Let’s clearly break down the three main network types in OpenShift:
Cluster Network, Service Network, and Host Network 👇
🧠 Overview
In OpenShift (and Kubernetes), networking is divided into logical networks — each with a distinct purpose:
| Network Type        | Purpose                                 | Who Uses It            | Example CIDR                     |
| ------------------- | --------------------------------------- | ---------------------- | -------------------------------- |
| **Cluster Network** | Communication between Pods              | Pods ↔ Pods            | `10.128.0.0/14`                  |
| **Service Network** | Virtual IPs for Services                | Pods ↔ Services        | `172.30.0.0/16`                  |
| **Host Network**    | Communication with the Node/Host itself | Nodes ↔ External World | Node IP, e.g. `192.168.122.0/24` |
Each layer has its own routing and isolation behavior, managed by the OpenShift SDN or OVN-Kubernetes network plugin.

🌐 1️⃣ Cluster Network
🔹 Definition
The Cluster Network is the pod network — where every pod gets its own IP address from this CIDR.
This network enables pod-to-pod communication across all nodes in the cluster.

🔹 How It Works
Each node gets a /23 or /24 subnet from the Cluster Network CIDR.
Example:
Node1 → 10.128.0.0/23
Node2 → 10.128.2.0/23
The OpenShift SDN/OVN plugin ensures:
Pods on the same node communicate locally via a virtual bridge.
Pods on different nodes communicate via overlay tunnels (VXLAN/Geneve).

🔹 Example CIDR
spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23

This means:
Pod IP range is 10.128.0.0 → 10.131.255.255
Each node can allocate /23 = 512 Pod IPs.

🔹 Used for
✅ Pod-to-Pod
✅ Pod-to-Service (through overlay routing)

🧩 2️⃣ Service Network
🔹 Definition
The Service Network is a virtual network used to assign ClusterIP addresses to Kubernetes Services.
These are not real interfaces on any node.
Services act as virtual load balancers for backend pods.

🔹 How It Works
When a Service is created, OpenShift allocates an IP from the Service Network CIDR.
kube-proxy (iptables) or OVN controller on each node installs rules to intercept packets to that IP and forward to backend pods.
The DNS name of the service (e.g., myapp.default.svc.cluster.local) resolves to this virtual IP.

🔹 Example CIDR
spec:
  serviceNetwork:
  - 172.30.0.0/16

🔹 Used for
✅ Pod → Service communication
✅ Route → Service communication

🔹 Example Flow
Pod (10.128.0.10) → Service IP (172.30.45.10)
       ↓
iptables/OVN NAT rules
       ↓
Forward to Pod backend (10.129.0.20)

🔹 Note
The Service network is internal only (not routable externally).
For external access, OpenShift provides Routes (or Ingress).

🖥️ 3️⃣ Host Network
🔹 Definition
The Host Network is the physical network on which the OpenShift nodes run.
This is how nodes communicate with:
Each other (for control plane, API, SDN tunnels)
The outside world (internet, NTP, repositories, users)

🔹 Example CIDR
192.168.122.0/24

🔹 Characteristics
Each node has a host interface (e.g., eth0) with an IP in this network.
SDN tunnels (VXLAN/Geneve) are established over the host network.
It’s completely separate from the Pod and Service networks.

🔹 Used for
✅ Node-to-Node communication
✅ Pod egress to external world (via SNAT on the node)
✅ User access to OpenShift API and Routes

📊 Comparison Summary Table
| Feature                  | **Cluster Network**            | **Service Network**           | **Host Network**        |
| ------------------------ | ------------------------------ | ----------------------------- | ----------------------- |
| **Purpose**              | Pod-to-Pod communication       | Service virtual IP routing    | Node-to-node & external |
| **Used By**              | Pods                           | Services & kube-proxy         | Nodes, API server       |
| **Example CIDR**         | `10.128.0.0/14`                | `172.30.0.0/16`               | `192.168.122.0/24`      |
| **Routable Externally?** | ❌ No                           | ❌ No                          | ✅ Yes                   |
| **Encapsulation**        | VXLAN / Geneve                 | NAT via iptables / OVN        | Plain (real network)    |
| **Isolation**            | Per namespace or NetworkPolicy | Per service                   | Physical segmentation   |
| **Defined In**           | `Network.config.openshift.io`  | `Network.config.openshift.io` | OS/Infrastructure level |

🔍 How to Check These in Your OpenShift Cluster
oc get network.config.openshift.io cluster -o yaml

🧩 1️⃣ Network Namespaces
🔹 Concept
In Linux (and Kubernetes/OpenShift), a network namespace provides an isolated network stack — like its own mini network world.
Each Pod in OpenShift runs inside a unique network namespace, meaning:
It has its own IP address, routing table, and interfaces.
It cannot see other pods’ interfaces or routes.
It behaves like a separate networked machine.

🔹 Example
When OpenShift (via CRI-O or containerd) creates a pod:
It sets up a network namespace for that pod.
It creates a virtual Ethernet pair (veth) to connect the pod to the node network.

You can see namespaces with:
ip netns list
You can view pod interfaces by entering the namespace:
nsenter -t <pod_pid> -n ip addr

🔌 2️⃣ Virtual Ethernet (veth) Pairs
🔹 Concept
A veth pair is like a virtual patch cable connecting two network namespaces:
One end is inside the pod’s namespace.
The other end is in the host (node) namespace, connected to a bridge or OVS switch.
When a packet leaves the pod, it goes out via its eth0 → veth → host bridge.

🔹 Example Flow
Pod Network Namespace
 └── eth0 (10.128.0.10)
      │
      │ veth pair
      │
Node Network Namespace
 └── vethXYZ connected to Open vSwitch (br-int)

🔹 Inside OpenShift
OpenShiftSDN or OVN-Kubernetes connects that host-side veth to Open vSwitch (OVS).
OVS then routes or encapsulates packets to the correct destination node (via VXLAN or Geneve).

You can list veth interfaces:
ip link show type veth
You can inspect connections:
ovs-vsctl show

📏 3️⃣ MTU (Maximum Transmission Unit)
🔹 Concept
MTU = The largest size (in bytes) of a packet that can be transmitted without fragmentation.
Typical defaults:
Ethernet (host network) → 1500 bytes
Overlay (VXLAN or Geneve) adds encapsulation overhead (≈ 50–60 bytes)
So, Pod network MTU must be smaller, usually 1450 or 1400.

🔹 Example Calculation (Geneve Encapsulation)
| Type               | Bytes          |
| ------------------ | -------------- |
| Ethernet Header    | 14             |
| IP Header          | 20             |
| UDP Header         | 8              |
| Geneve Header      | 38             |
| **Total Overhead** | **≈ 80 bytes** |

Hence:
1500 - 80 = 1420 MTU (typical pod MTU)

🔹 Why It Matters
If MTU is mismatched:
Packets get fragmented or dropped.
You see slow pod-to-pod communication or “connection reset” errors.
🔹 Check MTU Values

On pod:
ip link show eth0
On host:
ip link show br-int

🚀 4️⃣ Packet Flow Inside OpenShift Cluster
Let’s understand how a packet moves from one pod to another, step by step.

🟢 Case 1: Pod-to-Pod (Same Node)
Pod A sends a packet to Pod B (same node).
Packet leaves Pod A via eth0 → veth pair → OVS bridge (br-int).
OVS looks up Pod B’s MAC/IP in its flow table.
Delivers packet directly to Pod B’s veth (no encapsulation, very fast).

✅ No overlay or tunneling — local delivery.

🟠 Case 2: Pod-to-Pod (Different Nodes)
Pod A (on Node1) sends a packet to Pod B (on Node2).
Packet goes out Pod A’s veth → OVS bridge (br-int) on Node1.
OVS encapsulates packet using:
VXLAN (OpenShift SDN)
Geneve (OVN-Kubernetes)
Encapsulated packet travels over the Host Network (e.g., 192.168.122.0/24) to Node2.
Node2’s OVS decapsulates it.
Packet is injected into Pod B’s veth (delivered to Pod B).
So the outer IP header is Node1 → Node2, and the inner IP header is PodA → PodB.

🔵 Case 3: Pod-to-Service
Pod sends traffic to a Service IP (e.g., 172.30.x.x).
Kube-proxy or OVN rules intercept traffic to that virtual IP.
Service load-balancing picks one backend pod.
Packet gets DNAT’ed to the backend pod’s IP (10.128.x.x).
Routed through OVS bridge and overlay as in Case 1 or 2.

🔴 Case 4: Pod-to-External (Egress)
Pod sends traffic to an external IP (e.g., 8.8.8.8).
OVS forwards it to the node’s host interface.
The node performs SNAT (Pod IP → Node IP).
Packet exits the cluster via the host network.
Return packet comes back to the node and is DNATed back to the Pod.

🧭 Visual Summary (Text Diagram)
+--------------------------+
| Node 1                  |
| +--------------------+  |
| | Pod A (10.128.0.10)|  |
| |  eth0 ↔ vethA      |  |
| +--------------------+  |
|        │                 |
|   +----┴----+            |
|   |  OVS br-int|          |
|   +----┬----+            |
|        │ VXLAN/Geneve Encapsulation
|        ▼
|   Host Network (192.168.122.10)
+--------------------------+
             │
     Overlay Tunnel
             │
+--------------------------+
| Node 2                  |
| +--------------------+  |
| | Pod B (10.129.0.20)|  |
| |  eth0 ↔ vethB      |  |
| +--------------------+  |
|   OVS br-int          |
+--------------------------+

🧠 Key Takeaways
| Concept               | Purpose                                       | Where It Works                   |
| --------------------- | --------------------------------------------- | -------------------------------- |
| **Network Namespace** | Isolation per Pod                             | Kernel level                     |
| **veth Pair**         | Connects Pod ↔ Node                           | Bridge/OVS                       |
| **MTU**               | Packet size limit                             | Must account for overlay headers |
| **Packet Flow**       | Defines path from Pod to Pod/Service/External | Managed by SDN/OVN               |

